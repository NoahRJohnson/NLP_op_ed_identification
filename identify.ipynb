{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Op Ed Author\n",
    "\n",
    "In this directory are text files of the anonymous New York op ed from September 5, 2018, together with samples of writings of the main suspects. \n",
    "\n",
    "We will attempt to figure out who wrote the op ed using basic bigram language modeling techniques. We will construct a language model out of each author's writing, and then compare likelihoods on the op-ed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "AUTHOR_FOLDER = \"authors\"  # folder with training files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read in the text for every author.\"\"\"\n",
    "author_corpora = {}\n",
    "\n",
    "for fn in os.listdir(AUTHOR_FOLDER):\n",
    "    corpus_path = os.path.join(AUTHOR_FOLDER, fn)  # full path to corpus\n",
    "    author, ext = os.path.splitext(fn)  # remove extension from file name\n",
    "    try:\n",
    "        corpus_text = open(corpus_path).read()\n",
    "        #corpus_text = START_CHAR + corpus_text + END_CHAR  # add start and end characters\n",
    "        author_corpora[author] = dict(text=corpus_text)  # store text for this author\n",
    "    except:\n",
    "        print(\"ERROR: Could not read text file \\\"%s\\\"\" % corpus_path)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['testing', 'this', 'out', ',', 'what', 'is', 'up', '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"testing this out, what is up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"count unigram and bigram frequencies for each language\"\"\"\n",
    "for author in author_corpora.keys():\n",
    "    \n",
    "    # raw corpus text\n",
    "    corpus = author_corpora[author]['text']\n",
    "    \n",
    "    # tokenize corpus\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # Count unigrams and bigrams\n",
    "    unigram_counts = Counter(tokens)\n",
    "    bigram_counts = Counter(zip(tokens[:-1], tokens[1:]))\n",
    "    \n",
    "    N = sum(unigram_counts.values())  # Number of tokens in corpus\n",
    "    V = len(unigram_counts.keys())  # Vocabulary size, number of unique tokens in corpus\n",
    "    \n",
    "    # turn unigram counts into probabilities\n",
    "    unigram_probs = {unigram: count / float(N) \\\n",
    "                     for unigram,count in unigram_counts.iteritems()}\n",
    "    \n",
    "    # Store these properties in the dictionary for this language\n",
    "    train_corpora[language]['N'] = N\n",
    "    train_corpora[language]['V'] = V\n",
    "    \n",
    "    train_corpora[language]['unigram_counts'] = unigram_counts\n",
    "    train_corpora[language]['unigram_probs'] = unigram_probs\n",
    "    \n",
    "    train_corpora[language]['bigram_counts'] = bigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
